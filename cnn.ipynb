{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saulolks/CDSentimentAnalysis-Project/blob/master/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uE99A6Sn2nMJ",
        "colab_type": "code",
        "outputId": "e364eb2c-9f94-4788-bbc5-2599409f7ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GEMnmsPe2nl_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6Rhy2wrb7Fe3",
        "colab_type": "code",
        "outputId": "106c68db-9a8d-4df8-b899-790b7d8ebb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.backend import reshape\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-cLBfgkjDYsX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modelo Yoon Kim"
      ]
    },
    {
      "metadata": {
        "id": "P-Gk_f1NwDf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modelo original de **Yoon Kim** [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
        "\n",
        "Implementação [Jverma/cnn-text-classification-keras\n",
        "](https://github.com/Jverma/cnn-text-classification-keras)"
      ]
    },
    {
      "metadata": {
        "id": "YNWh9i9cw6Ia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*   **EMBEDDING_DIM**: Dimension of the embedding space.\n",
        "*   **MAX_SEQUENCE_LENGTH:** Maximum length of the sentence.\n",
        "*   **MAX_NB_WORDS:** Maximum number of words in the vocabulary.\n",
        "*   **embeddings_index:** A dict containing words and their embeddings.\n",
        "*   **word_index:** A dict containing words and their indices.\n",
        "*   **labels_index**: A dict containing the labels and their indices.\n",
        "*   **fname**: Path to the file containing Google word2vecs."
      ]
    },
    {
      "metadata": {
        "id": "Yom2jf-MW3MN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datafolder = 'drive/My Drive/Colab Notebooks/MT/imdb_dataSet_k.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H14IvPE4t23i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 6118\n",
        "MAX_NB_WORDS = 10000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.2\n",
        "fname = 'drive/My Drive/UFRPE/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KLOf9XUeZOms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Funções"
      ]
    },
    {
      "metadata": {
        "id": "tDNZ9-3KYiyN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepareDataSet(datapath):\n",
        "    #Shuffle\n",
        "    data = pd.read_csv(datapath) \n",
        "    data = data.sample(frac=1)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3WFIbE4gYjxK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = prepareDataSet(datafolder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwWGmmmLaugs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = len(data.columns)-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VvQBNnRcW2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = data.iloc[:,2:len(data.columns)-1]\n",
        "Y = data[\"class\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CzHu1oZ9qUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Modelo"
      ]
    },
    {
      "metadata": {
        "id": "cJoZiCC9DaSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(num_words, EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5zBJHBm7Z2v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "# add first conv filter\n",
        "embedded_sequences = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
        "x = Conv2D(100, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
        "x = MaxPooling2D((MAX_SEQUENCE_LENGTH - 5 + 1, 1))(x)\n",
        "\n",
        "\n",
        "# add second conv filter.\n",
        "y = Conv2D(100, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
        "y = MaxPooling2D((MAX_SEQUENCE_LENGTH - 4 + 1, 1))(y)\n",
        "\n",
        "\n",
        "# add third conv filter.\n",
        "z = Conv2D(100, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
        "z = MaxPooling2D((MAX_SEQUENCE_LENGTH - 3 + 1, 1))(z)\n",
        "\n",
        "\n",
        "# concate the conv layers\n",
        "alpha = concatenate([x,y,z])\n",
        "\n",
        "# flatted the pooled features.\n",
        "alpha = Flatten()(alpha)\n",
        "\n",
        "# dropout\n",
        "alpha = Dropout(0.5)(alpha)\n",
        "\n",
        "# predictions\n",
        "preds = Dense(2, activation='softmax')(alpha)\n",
        "\n",
        "# build model\n",
        "model = Model(sequence_input, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uvvz4XcL7e_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "adadelta = optimizers.Adadelta()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adadelta,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTYJryj-9jhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ]
    },
    {
      "metadata": {
        "id": "u3sMSvl9fxdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "36fdfc75-992e-43b7-eb22-0c609a054558"
      },
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=10, shuffle=False)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
        "    \n",
        "    model.fit(x=x_train, y=y_train, batch_size=50, epochs=5)\n",
        "    \n",
        "    break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f525ed6afe04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,)"
          ]
        }
      ]
    }
  ]
}